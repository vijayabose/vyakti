# Vyakti: Rust Implementation of LEANN Vector Database

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Rust](https://img.shields.io/badge/rust-1.70%2B-orange.svg)](https://www.rust-lang.org/)

**Vyakti** is a high-performance Rust implementation of LEANN (Low-Storage Vector Index), a vector database that achieves 97% storage savings through graph-based selective recomputation.

## Project Status

üöß **IN DEVELOPMENT** - Migrating LEANN from Python to Rust for improved performance, memory safety, and cross-platform deployment.

## Overview

Vyakti is designed as a **complete product** that can be used in three modes:

1. **üì¶ Library** - Import into your Rust projects via Cargo
2. **‚ö° CLI Tool** - Standalone command-line application
3. **üåê Server** - Network-accessible API server with REST/gRPC endpoints

### Key Features

- **üî• Blazing Fast** - 10-100x faster than Python implementation
- **üíæ Ultra-Low Storage** - 97% storage reduction vs traditional vector databases
- **üîí Memory Safe** - Rust's ownership model prevents common bugs
- **üîå Plugin Architecture** - Modular backend system (HNSW, DiskANN)
- **üåç Cross-Platform** - Linux, macOS, Windows, ARM64 support
- **üöÄ Zero-Copy** - Efficient memory-mapped file operations
- **‚ö° Async/Await** - Non-blocking I/O for server mode
- **üìä Observability** - Built-in metrics, tracing, and logging

## Table of Contents

- [Quick Start](#quick-start)
- [Installation](#installation)
- [Usage](#usage)
  - [As a Library](#as-a-library)
  - [As a CLI](#as-a-cli)
  - [As a Server](#as-a-server)
- [Architecture](#architecture)
- [Features](#features)
- [Performance](#performance)
- [Configuration](#configuration)
- [Development](#development)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [License](#license)

## Quick Start

### Install CLI

```bash
# Install from crates.io
cargo install vyakti

# Or build from source
git clone https://github.com/yourusername/vyakti.git
cd vyakti
cargo install --path crates/vyakti-cli
```

### Build and Search an Index

```bash
# Create an index from documents
leann build my-docs --input ./documents --model all-MiniLM-L6-v2

# Search the index
leann search my-docs "vector database concepts" --top-k 10

# Interactive chat mode
leann chat my-docs --interactive

# List all indexes
leann list

# Start server mode
leann serve --port 8080 --host 0.0.0.0
```

### Use as a Library

```rust
use leann_core::{LeannBuilder, LeannSearcher, EmbeddingModel};
use leann_backend_hnsw::HnswBackend;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // Create a new index builder
    let mut builder = LeannBuilder::new()
        .backend(HnswBackend::default())
        .embedding_model(EmbeddingModel::SentenceTransformers {
            model_name: "all-MiniLM-L6-v2".to_string(),
        })
        .build();

    // Add documents
    builder.add_text("LEANN is a vector database", None).await?;
    builder.add_text("Rust is a systems programming language", None).await?;

    // Build the index
    let index_path = builder.build_index("my-index").await?;

    // Search the index
    let searcher = LeannSearcher::from_path(&index_path)?;
    let results = searcher.search("database", 5).await?;

    for result in results {
        println!("Text: {}, Score: {}", result.text, result.score);
    }

    Ok(())
}
```

## Installation

### Prerequisites

- Rust 1.70 or later
- LLVM/Clang (for some backends)
- OpenMP (optional, for parallel processing)

### Platform-Specific Requirements

#### macOS

```bash
brew install llvm libomp cmake
```

#### Ubuntu/Debian

```bash
sudo apt-get install build-essential cmake clang libomp-dev pkg-config
```

#### Windows

```bash
# Using chocolatey
choco install llvm cmake
```

### Install from Source

```bash
git clone https://github.com/yourusername/vyakti.git
cd vyakti

# Build all crates
cargo build --release --workspace

# Run tests
cargo test --workspace

# Install CLI globally
cargo install --path crates/vyakti-cli
```

## Usage

### As a Library

Add to your `Cargo.toml`:

```toml
[dependencies]
vyakti-core = "0.1.0"
vyakti-backend-hnsw = "0.1.0"  # Or other backends
tokio = { version = "1", features = ["full"] }
```

#### Basic Example

```rust
use leann_core::prelude::*;

#[tokio::main]
async fn main() -> Result<()> {
    let mut builder = LeannBuilder::new()
        .with_backend("hnsw")
        .with_embedding_model("all-MiniLM-L6-v2");

    builder.add_texts(vec![
        "Document 1 content",
        "Document 2 content",
    ]).await?;

    let index = builder.build().await?;
    let results = index.search("query", 10).await?;

    Ok(())
}
```

#### Advanced Features

```rust
use leann_core::{LeannBuilder, MetadataFilter, SearchOptions};

// With metadata filtering
let results = searcher.search_with_options(
    "machine learning",
    SearchOptions {
        top_k: 20,
        metadata_filter: Some(MetadataFilter::And(vec![
            MetadataFilter::Eq("category".into(), "AI".into()),
            MetadataFilter::Gt("year".into(), 2020.into()),
        ])),
        ..Default::default()
    }
).await?;

// With recomputation disabled for maximum speed
let fast_index = LeannBuilder::new()
    .disable_recomputation()
    .build();
```

### As a CLI

#### Index Management

```bash
# Build index from files
leann build docs --input ./documents --recursive

# Build with specific backend
leann build code --input ./src --backend diskann --enable-code-chunking

# Build with custom parameters
leann build data \
    --input ./data \
    --model Qwen/Qwen3-Embedding-0.6B \
    --graph-degree 64 \
    --chunk-size 512 \
    --chunk-overlap 50
```

#### Search Operations

```bash
# Simple search
leann search docs "vector database"

# Advanced search with metadata
leann search docs "machine learning" \
    --top-k 20 \
    --metadata-filter 'category == "AI" AND year > 2020'

# Export results to JSON
leann search docs "query" --format json > results.json
```

#### Server Management

```bash
# Start server with default settings
leann serve

# Production server with custom config
leann serve \
    --host 0.0.0.0 \
    --port 8080 \
    --workers 4 \
    --max-connections 1000 \
    --enable-tls \
    --cert ./server.crt \
    --key ./server.key
```

### As a Server

#### REST API

Start the server:

```bash
leann serve --port 8080
```

Example API requests:

```bash
# Health check
curl http://localhost:8080/health

# Build index
curl -X POST http://localhost:8080/api/v1/indexes \
  -H "Content-Type: application/json" \
  -d '{
    "name": "my-docs",
    "backend": "hnsw",
    "embedding_model": "all-MiniLM-L6-v2"
  }'

# Add documents
curl -X POST http://localhost:8080/api/v1/indexes/my-docs/documents \
  -H "Content-Type: application/json" \
  -d '{
    "documents": [
      {"text": "Document 1", "metadata": {"category": "tech"}},
      {"text": "Document 2", "metadata": {"category": "science"}}
    ]
  }'

# Search
curl -X POST http://localhost:8080/api/v1/indexes/my-docs/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "vector database",
    "top_k": 10,
    "metadata_filter": {"category": {"$eq": "tech"}}
  }'

# List indexes
curl http://localhost:8080/api/v1/indexes
```

#### gRPC API

```rust
use leann_grpc::LeannClient;

#[tokio::main]
async fn main() -> Result<()> {
    let mut client = LeannClient::connect("http://localhost:50051").await?;

    let response = client.search(SearchRequest {
        index_name: "my-docs".to_string(),
        query: "vector database".to_string(),
        top_k: 10,
        ..Default::default()
    }).await?;

    for result in response.into_inner().results {
        println!("{}: {}", result.score, result.text);
    }

    Ok(())
}
```

## Architecture

```
vyakti/
‚îú‚îÄ‚îÄ crates/
‚îÇ   ‚îú‚îÄ‚îÄ vyakti-core/          # Core library (Builder, Searcher, API)
‚îÇ   ‚îú‚îÄ‚îÄ vyakti-backend-hnsw/  # HNSW backend implementation
‚îÇ   ‚îú‚îÄ‚îÄ vyakti-backend-diskann/ # DiskANN backend implementation
‚îÇ   ‚îú‚îÄ‚îÄ vyakti-embedding/     # Embedding computation layer
‚îÇ   ‚îú‚îÄ‚îÄ vyakti-server/        # REST & gRPC server
‚îÇ   ‚îú‚îÄ‚îÄ vyakti-cli/           # Command-line interface
‚îÇ   ‚îú‚îÄ‚îÄ vyakti-storage/       # Storage layer (CSR, memory mapping)
‚îÇ   ‚îú‚îÄ‚îÄ vyakti-proto/         # Protocol buffers definitions
‚îÇ   ‚îî‚îÄ‚îÄ vyakti-common/        # Shared utilities and types
‚îú‚îÄ‚îÄ benches/                 # Performance benchmarks
‚îú‚îÄ‚îÄ examples/                # Example applications
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îî‚îÄ‚îÄ tests/                   # Integration tests
```

### Module Responsibilities

| Crate | Purpose | Exports |
|-------|---------|---------|
| `vyakti-core` | Main API surface | `LeannBuilder`, `LeannSearcher`, `LeannChat` |
| `leann-backend-*` | Vector search backends | Backend implementations |
| `vyakti-embedding` | Embedding models | `EmbeddingModel`, `EmbeddingServer` |
| `vyakti-server` | Network server | REST/gRPC endpoints |
| `vyakti-cli` | CLI interface | Binary executable |
| `vyakti-storage` | Persistence layer | CSR format, memory mapping |
| `vyakti-common` | Shared utilities | Error types, config, traits |

## Features

### Core Features

- ‚úÖ **Graph-Based Recomputation** - Store graph structure, recompute embeddings on-demand
- ‚úÖ **Multiple Backends** - HNSW, DiskANN with pluggable architecture
- ‚úÖ **Embedding Models**
  - Local: SentenceTransformers, Ollama
  - Cloud: OpenAI, Cohere, Voyage
  - Custom: ONNX Runtime support
- ‚úÖ **Metadata Filtering** - SQL-like queries with rich operators
- ‚úÖ **AST-Aware Chunking** - Language-aware code chunking (Python, Java, C#, TypeScript, Rust)
- ‚úÖ **Hybrid Search** - Combine semantic + keyword search
- ‚úÖ **Streaming APIs** - Stream large result sets
- ‚úÖ **Compression** - PQ (Product Quantization) support

### Advanced Features

- ‚úÖ **Zero-Copy Operations** - Memory-mapped files for efficient loading
- ‚úÖ **Async/Await** - Non-blocking I/O throughout
- ‚úÖ **Multi-Threading** - Parallel index building and search
- ‚úÖ **Hot Reload** - Update indexes without downtime
- ‚úÖ **Incremental Updates** - Add/remove documents without rebuild
- ‚úÖ **Snapshots** - Point-in-time index backups
- ‚úÖ **Observability**
  - Structured logging (tracing)
  - Prometheus metrics
  - OpenTelemetry support
  - Health checks and readiness probes

### Server Features

- ‚úÖ **REST API** - JSON-based HTTP endpoints
- ‚úÖ **gRPC API** - High-performance binary protocol
- ‚úÖ **Authentication** - API keys, JWT tokens
- ‚úÖ **Rate Limiting** - Per-user/per-IP limits
- ‚úÖ **TLS Support** - Encrypted connections
- ‚úÖ **Load Balancing** - Multi-instance deployment
- ‚úÖ **Horizontal Scaling** - Distributed indexes (planned)

## Performance

### Benchmarks (vs Python Implementation)

| Operation | Python | Rust (Vyakti) | Speedup |
|-----------|--------|-----------------|---------|
| Index Build (1M docs) | 180s | 12s | **15x** |
| Search (Top-10) | 45ms | 0.8ms | **56x** |
| Embedding Compute | 120ms | 8ms | **15x** |
| Index Load Time | 2.3s | 0.05s | **46x** |
| Memory Usage | 4.2GB | 0.8GB | **5.2x less** |

### Storage Comparison

| Backend | Full Vectors | LEANN Compact | Savings |
|---------|--------------|---------------|---------|
| HNSW | 512MB | 15MB | **96.7%** |
| DiskANN | 512MB | 28MB | **94.5%** |

## Configuration

### Configuration File

Create `leann.toml` in your project:

```toml
[index]
name = "my-index"
backend = "hnsw"
dimension = 384

[embedding]
provider = "sentence-transformers"
model = "all-MiniLM-L6-v2"
batch_size = 32
device = "cuda"  # or "cpu", "mps"

[hnsw]
graph_degree = 32
build_complexity = 64
search_complexity = 32

[storage]
compact = true
recompute = true
memory_map = true

[server]
host = "0.0.0.0"
port = 8080
workers = 4
max_connections = 1000

[server.tls]
enabled = false
cert_path = "./server.crt"
key_path = "./server.key"

[logging]
level = "info"
format = "json"  # or "pretty"
```

### Environment Variables

```bash
# Server settings
LEANN_HOST=0.0.0.0
LEANN_PORT=8080
LEANN_WORKERS=4

# API keys
OPENAI_API_KEY=sk-...
COHERE_API_KEY=...

# Logging
RUST_LOG=leann=debug,tower_http=info
RUST_BACKTRACE=1
```

## Development

### Prerequisites

```bash
# Install Rust toolchain
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Install development tools
cargo install cargo-watch cargo-edit cargo-outdated
```

### Build and Test

```bash
# Development build
cargo build

# Release build with optimizations
cargo build --release

# Run tests
cargo test --workspace

# Run with examples
cargo run --example basic_search

# Watch mode for development
cargo watch -x "test --workspace"
```

### Code Quality

```bash
# Format code
cargo fmt --all

# Lint code
cargo clippy --all-targets --all-features

# Check for security vulnerabilities
cargo audit

# Generate documentation
cargo doc --no-deps --open
```

### Benchmarking

```bash
# Run all benchmarks
cargo bench --workspace

# Specific benchmark
cargo bench --bench search_performance

# With flamegraph profiling
cargo flamegraph --bench search_performance
```

## Roadmap

### Phase 1: Core Migration (Q1 2025)
- [ ] Core data structures (LeannBuilder, LeannSearcher)
- [ ] HNSW backend with CSR storage
- [ ] Basic embedding support (SentenceTransformers)
- [ ] CLI tool with build/search commands
- [ ] Unit and integration tests

### Phase 2: Advanced Features (Q2 2025)
- [ ] DiskANN backend with PQ compression
- [ ] Metadata filtering engine
- [ ] AST-aware code chunking
- [ ] Hybrid search (semantic + keyword)
- [ ] Incremental updates

### Phase 3: Server & Production (Q3 2025)
- [ ] REST API server
- [ ] gRPC API
- [ ] Authentication and authorization
- [ ] Metrics and observability
- [ ] Docker images and Kubernetes manifests

### Phase 4: Scale & Optimization (Q4 2025)
- [ ] Distributed indexes
- [ ] Horizontal scaling
- [ ] Advanced compression techniques
- [ ] GPU acceleration
- [ ] WASM compilation for web

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Workflow

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and add tests
4. Run tests and linting: `cargo test && cargo clippy`
5. Commit your changes: `git commit -m 'Add amazing feature'`
6. Push to the branch: `git push origin feature/amazing-feature`
7. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Original LEANN paper: [arXiv:2506.08276](https://arxiv.org/abs/2506.08276)
- Python implementation: [LEANN](https://github.com/yichuan-w/LEANN)
- HNSW algorithm: [hnswlib](https://github.com/nmslib/hnswlib)
- DiskANN: [Microsoft DiskANN](https://github.com/microsoft/DiskANN)

## Support

- üìß Email: support@vyakti.dev
- üí¨ Discord: [Join our community](https://discord.gg/vyakti)
- üêõ Issues: [GitHub Issues](https://github.com/yourusername/vyakti/issues)
- üìñ Docs: [https://docs.vyakti.dev](https://docs.vyakti.dev)

## Citation

If you use Vyakti in your research, please cite:

```bibtex
@article{leann2024,
  title={LEANN: Low-Storage Vector Index with Graph-Based Selective Recomputation},
  author={Wang, Yichuan},
  journal={arXiv preprint arXiv:2506.08276},
  year={2024}
}
```
